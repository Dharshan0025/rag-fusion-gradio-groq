# -*- coding: utf-8 -*-
"""Fusion_Rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z6f1ZzfMEOCLf2SAzWZhagL-SpbHc1-l
"""

pip install faiss-cpu rank-bm25 langchain-groq langchain-community

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from rank_bm25 import BM25Okapi
from langchain_groq import ChatGroq
from google.colab import userdata
import numpy as np
import gradio as gr

# ------------------------------
# 1. Load & split documents
# ------------------------------
loader = TextLoader("notes.txt")   # replace with your own file
documents = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = splitter.split_documents(documents)

# ------------------------------
# 2. Create FAISS Vector Store
# ------------------------------
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
faiss_store = FAISS.from_documents(docs, embeddings)

# ------------------------------
# 3. BM25 Retriever
# ------------------------------
bm25 = BM25Okapi([d.page_content.split() for d in docs])

# ------------------------------
# 4. Reciprocal Rank Fusion
# ------------------------------
def reciprocal_rank_fusion(results, k=60):
    fused_scores = {}
    for retriever_results in results:
        for rank, doc in enumerate(retriever_results):
            if doc.page_content not in fused_scores:
                fused_scores[doc.page_content] = 0
            fused_scores[doc.page_content] += 1 / (rank + k)
    # Sort by fused score
    ranked = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)
    return [doc for doc, _ in ranked]

# ------------------------------
# 5. Groq LLM
# ------------------------------
llm = ChatGroq(api_key=userdata.get('GROQ_API_KEY'), model="llama-3.3-70b-versatile")

def rag_answer(query):
    # Get FAISS results
    faiss_results = faiss_store.similarity_search(query, k=5)

    # Get BM25 results
    bm25_scores = bm25.get_scores(query.split())
    top_idx = np.argsort(bm25_scores)[::-1][:5]
    bm25_results = [docs[i] for i in top_idx]

    # Fuse results
    fused_docs = reciprocal_rank_fusion([faiss_results, bm25_results])

    # Build context
    context = "\n\n".join([doc for doc in fused_docs[:3]])

    prompt = f"""
    Answer the question based only on the context below.
    If you don't know, say "I don't know."

    Context:
    {context}

    Question: {query}
    """

    response = llm.invoke(prompt)
    return response.content

# ------------------------------
# 6. Gradio Interface
# ------------------------------
with gr.Blocks() as demo:
    gr.Markdown("## ðŸ§  RAG-Fusion Chatbot (Groq + HuggingFace + FAISS + RRF)")
    chatbot = gr.Chatbot()
    msg = gr.Textbox(placeholder="Ask me anything...")
    clear = gr.Button("Clear Chat")

    def user_query(user_msg, history):
        answer = rag_answer(user_msg)
        history.append((user_msg, answer))
        return history, ""

    msg.submit(user_query, [msg, chatbot], [chatbot, msg])
    clear.click(lambda: [], None, chatbot)

demo.launch()